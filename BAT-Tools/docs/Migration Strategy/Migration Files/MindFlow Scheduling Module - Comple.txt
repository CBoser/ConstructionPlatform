MindFlow Scheduling Module - Complete Specification
I'll create all four components for your construction technology platform.

A) MindFlow Scheduling Module Specification
1. Executive Summary
Purpose: Centralized scheduling system for construction takeoff delivery, crew assignment, and material coordination across Richmond American (61 plans), Holt Homes, Sekisui House, and future builders.
Core Problem Solved: Eliminates scattered Excel-based scheduling, manual crew assignment, and lack of visibility into delivery capacity and material staging.
Key Outcomes:

Reduce takeoff processing time from 10-15 min to 1-2 min
Automated crew assignment based on capacity and location
Real-time visibility into delivery schedules
Seamless integration with existing BAT/RF/MaterialFlow systems


2. System Architecture
┌─────────────────────────────────────────────────────────────┐
│                    MindFlow Platform                         │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │     BAT      │  │  ReadyFrame  │  │ MaterialFlow │     │
│  │   (Orders)   │  │  (Takeoffs)  │  │ (Procurement)│     │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘     │
│         │                  │                  │             │
│         └──────────────────┼──────────────────┘             │
│                            │                                │
│                  ┌─────────▼─────────┐                     │
│                  │   SCHEDULING      │                     │
│                  │     MODULE        │                     │
│                  └─────────┬─────────┘                     │
│                            │                                │
│         ┌──────────────────┼──────────────────┐            │
│         │                  │                  │            │
│    ┌────▼────┐      ┌─────▼─────┐      ┌────▼────┐       │
│    │  Jobs   │      │ Schedules │      │  Crews  │       │
│    │Database │      │ Database  │      │Database │       │
│    └─────────┘      └───────────┘      └─────────┘       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
         │                  │                  │
         ▼                  ▼                  ▼
    ┌────────┐        ┌──────────┐      ┌──────────┐
    │Richmond│        │   Holt   │      │ Sekisui  │
    │American│        │  Homes   │      │  House   │
    └────────┘        └──────────┘      └──────────┘

3. Core Features
3.1 Job Management

Create jobs from BAT orders or RF takeoffs
Track job status through lifecycle
Link to builder-specific requirements
GPS/mapping integration for routing

3.2 Intelligent Scheduling

Automated crew assignment based on:

Geographic zones (minimize drive time)
Crew capacity (weight, volume, job count)
Skill requirements (framing, trusses, specialty)
Builder priority levels


Date/time window management
Route optimization (multiple jobs per day)

3.3 Resource Management

Crew availability tracking
Equipment assignment
Truck/vehicle capacity planning
Material staging coordination

3.4 Builder Integration

Richmond American: BAT migration (61 plans)
Holt Homes: CSV import + Random Lengths pricing
Sekisui House: Custom workflow requirements
Extensible for new builders

3.5 Analytics & Reporting

Crew utilization rates
On-time delivery metrics
Cost per delivery
Builder-specific performance dashboards


4. User Roles & Permissions
RolePermissionsAdminFull system access, configuration, user managementSchedulerCreate/edit schedules, assign crews, view all jobsCrew LeadView assigned jobs, update job status, report issuesBuilder ContactView jobs for their builder, request changesReadonlyDashboard access, reporting only

5. Workflow: End-to-End
1. ORDER RECEIVED
   └─> BAT system captures order details
   └─> RF generates takeoff (walls, roof, materials)
   └─> MaterialFlow checks inventory

2. JOB CREATION
   └─> Scheduling Module creates Job record
   └─> Validates address, calculates GPS coordinates
   └─> Determines zone, assigns priority

3. SCHEDULING
   └─> Scheduler reviews pending jobs
   └─> System suggests optimal crew + date
   └─> Scheduler confirms or adjusts
   └─> Schedule record created

4. ROUTE OPTIMIZATION
   └─> System groups jobs by date + zone
   └─> Calculates optimal delivery sequence
   └─> Assigns route position to each job

5. CREW ASSIGNMENT
   └─> Crew receives schedule via mobile/email
   └─> Views job details, site access notes
   └─> Updates status: en route → arrived → completed

6. COMPLETION & INVOICING
   └─> Crew marks job complete
   └─> Photos/notes uploaded
   └─> Triggers invoice generation in BAT
   └─> Customer notification sent

6. Technical Requirements
Backend:

Language: Python (FastAPI or Django REST)
Database: PostgreSQL (complex queries, GIS support)
Caching: Redis (for route optimization results)
Queue: Celery (for async job processing)

Frontend:

Framework: React (consistent with your existing artifacts)
Mapping: Mapbox or Google Maps API
State Management: Redux or Zustand
UI Library: Tailwind CSS + shadcn/ui

Integration:

Excel Import/Export: openpyxl or pandas
CSV Processing: pandas
PDF Generation: ReportLab or WeasyPrint
SharePoint: Microsoft Graph API
Email/SMS: SendGrid, Twilio


7. Key Business Rules
Crew Assignment Logic:
pythondef assign_crew(job):
    # 1. Zone matching (highest priority)
    eligible_crews = filter_by_zone(job.zone)
    
    # 2. Capacity check
    eligible_crews = filter_by_capacity(eligible_crews, job.weight, job.volume)
    
    # 3. Skill requirements
    if job.requires_crane:
        eligible_crews = filter_by_skill(eligible_crews, 'crane_certified')
    
    # 4. Availability check
    eligible_crews = filter_by_availability(eligible_crews, job.requested_date)
    
    # 5. Load balancing (prefer crews with fewer assignments)
    return select_least_loaded(eligible_crews)
```

#### **Priority Levels:**
| Priority | Description | SLA |
|----------|-------------|-----|
| **RUSH** | Emergency/same-day | 4 hours |
| **PREMIUM** | Builder priority tier | 24 hours |
| **STANDARD** | Normal delivery | 3-5 days |
| **BACKLOG** | Flexible timing | 1-2 weeks |

#### **Scheduling Windows:**
- **Morning:** 06:00-10:00 (preferred for framing)
- **Midday:** 10:00-14:00
- **Afternoon:** 14:00-18:00
- **Custom:** Builder-specific requirements

---

### **8. Richmond American BAT Migration**

**Current State:** 61 building plans in legacy Excel BAT system

**Migration Strategy:**
```
Phase 1: Data Extraction (Week 1)
├─ Extract all 61 Excel files
├─ Parse plan metadata (plan name, SF, configuration)
├─ Extract takeoff data (walls, roof, materials)
└─ Validate data integrity

Phase 2: Data Transformation (Week 2)
├─ Map Excel columns to new schema
├─ Generate Job records for each plan
├─ Create standard material lists per plan
└─ Calculate default capacities

Phase 3: System Import (Week 3)
├─ Import Jobs via API
├─ Link to Richmond American builder profile
├─ Create template schedules
└─ Generate reports for validation

Phase 4: Parallel Testing (Week 4)
├─ Run both systems simultaneously
├─ Compare outputs for accuracy
├─ Train schedulers on new system
└─ Address any discrepancies

Phase 5: Cutover (Week 5)
├─ Final data sync
├─ Decommission legacy Excel system
├─ Full production launch
└─ Monitor and support

9. Success Metrics
MetricCurrentTargetTimeframeTakeoff Processing Time10-15 min1-2 min3 monthsScheduling EfficiencyManual, 30 min/jobAutomated, 5 min/job3 monthsOn-Time Delivery RateUnknown>95%6 monthsCrew UtilizationUnknown>85%6 monthsSystem Consolidation30+ scattered tools1 platform12 months

10. Risks & Mitigation
RiskImpactLikelihoodMitigationData quality issues in legacy ExcelHighHighExtensive validation, manual reviewCrew adoption resistanceMediumMediumTraining, mobile-first design, incentivesGPS/mapping inaccuracyMediumLowManual address validation, site notesBuilder-specific requirements conflictLowMediumConfigurable business rules per builderIntegration failures with existing systemsHighLowRobust error handling, fallback workflows

B) Database Schemas
Schema Overview
sql-- PostgreSQL with PostGIS extension for geographic queries

-- ============================================================
-- BUILDERS & ORGANIZATIONS
-- ============================================================

CREATE TABLE builders (
    builder_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    short_code VARCHAR(50) NOT NULL UNIQUE, -- e.g., 'RHAM', 'HOLT', 'SEKI'
    priority_level VARCHAR(50) DEFAULT 'STANDARD', -- RUSH, PREMIUM, STANDARD, BACKLOG
    contact_name VARCHAR(255),
    contact_email VARCHAR(255),
    contact_phone VARCHAR(50),
    default_schedule_window VARCHAR(50), -- e.g., 'MORNING', 'AFTERNOON'
    notes TEXT,
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE communities (
    community_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    builder_id UUID REFERENCES builders(builder_id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    address VARCHAR(500),
    city VARCHAR(100),
    state VARCHAR(2),
    zip VARCHAR(10),
    zone_code VARCHAR(50), -- Geographic zone for routing
    gps_lat DECIMAL(10, 8),
    gps_lon DECIMAL(11, 8),
    site_access_notes TEXT,
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ============================================================
-- PLANS & CONFIGURATIONS
-- ============================================================

CREATE TABLE building_plans (
    plan_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    builder_id UUID REFERENCES builders(builder_id) ON DELETE CASCADE,
    plan_name VARCHAR(255) NOT NULL,
    plan_code VARCHAR(100), -- e.g., 'RES2-2450'
    square_footage INTEGER,
    stories DECIMAL(3, 1), -- 1.0, 1.5, 2.0, etc.
    bedrooms INTEGER,
    bathrooms DECIMAL(3, 1),
    garage_bays INTEGER,
    description TEXT,
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT unique_builder_plan UNIQUE(builder_id, plan_code)
);

CREATE TABLE plan_configurations (
    config_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    plan_id UUID REFERENCES building_plans(plan_id) ON DELETE CASCADE,
    config_name VARCHAR(255) NOT NULL, -- e.g., 'Standard', 'With Bonus Room'
    walls_count INTEGER,
    roof_trusses_count INTEGER,
    estimated_linear_feet INTEGER,
    estimated_weight_lbs INTEGER,
    estimated_volume_cuft INTEGER,
    estimated_duration_minutes INTEGER DEFAULT 240, -- 4 hours default
    material_list_json JSONB, -- Detailed material breakdown
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT unique_plan_config UNIQUE(plan_id, config_name)
);

-- ============================================================
-- JOBS (Core Scheduling Entity)
-- ============================================================

CREATE TABLE jobs (
    job_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_number VARCHAR(100) UNIQUE NOT NULL, -- e.g., 'SA-2024-001'
    builder_id UUID REFERENCES builders(builder_id),
    community_id UUID REFERENCES communities(community_id),
    plan_id UUID REFERENCES building_plans(plan_id),
    config_id UUID REFERENCES plan_configurations(config_id),
    
    -- Address/Location
    street_address VARCHAR(500) NOT NULL,
    city VARCHAR(100) NOT NULL,
    state VARCHAR(2) NOT NULL,
    zip VARCHAR(10) NOT NULL,
    lot_number VARCHAR(50),
    gps_lat DECIMAL(10, 8),
    gps_lon DECIMAL(11, 8),
    zone_code VARCHAR(50),
    
    -- Job Details
    job_type VARCHAR(100) DEFAULT 'COMPLETE_FRAMING', -- WALLS_ONLY, ROOF_ONLY, COMPLETE_FRAMING
    priority_level VARCHAR(50) DEFAULT 'STANDARD',
    status VARCHAR(50) DEFAULT 'PENDING', -- PENDING, SCHEDULED, IN_PROGRESS, COMPLETED, CANCELLED, ON_HOLD
    
    -- Capacity/Requirements
    estimated_weight_lbs INTEGER,
    estimated_volume_cuft INTEGER,
    estimated_duration_minutes INTEGER DEFAULT 240,
    requires_crane BOOLEAN DEFAULT FALSE,
    requires_forklift BOOLEAN DEFAULT FALSE,
    special_requirements TEXT,
    
    -- Scheduling
    requested_date DATE,
    requested_time_window VARCHAR(50), -- e.g., 'MORNING', '08:00-12:00'
    flexible_scheduling BOOLEAN DEFAULT TRUE,
    
    -- Customer/Contact
    customer_name VARCHAR(255),
    customer_phone VARCHAR(50),
    customer_email VARCHAR(255),
    on_site_contact VARCHAR(255),
    on_site_phone VARCHAR(50),
    
    -- Site Access
    site_access_notes TEXT,
    delivery_instructions TEXT,
    site_restrictions TEXT, -- e.g., 'No deliveries before 8am'
    
    -- Financial
    estimated_cost DECIMAL(10, 2),
    actual_cost DECIMAL(10, 2),
    invoiced BOOLEAN DEFAULT FALSE,
    invoice_number VARCHAR(100),
    
    -- Integration
    bat_order_id VARCHAR(100), -- Link to BAT system
    rf_takeoff_id VARCHAR(100), -- Link to ReadyFrame
    materialflow_po_id VARCHAR(100), -- Link to MaterialFlow
    external_reference VARCHAR(255),
    
    -- Audit
    created_by UUID, -- User ID
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    cancelled_at TIMESTAMP,
    cancellation_reason TEXT
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_builder ON jobs(builder_id);
CREATE INDEX idx_jobs_requested_date ON jobs(requested_date);
CREATE INDEX idx_jobs_zone ON jobs(zone_code);
CREATE INDEX idx_jobs_created_at ON jobs(created_at);

-- ============================================================
-- SCHEDULES
-- ============================================================

CREATE TABLE schedules (
    schedule_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES jobs(job_id) ON DELETE CASCADE,
    
    -- Schedule Details
    scheduled_date DATE NOT NULL,
    scheduled_time_start TIME,
    scheduled_time_end TIME,
    scheduled_window VARCHAR(50), -- MORNING, MIDDAY, AFTERNOON, CUSTOM
    
    -- Route Information
    route_id UUID, -- Groups jobs delivered together
    route_position INTEGER, -- Order in route (1st stop, 2nd stop, etc.)
    estimated_arrival_time TIME,
    estimated_departure_time TIME,
    
    -- Assignment
    crew_id UUID, -- References crews table
    truck_id UUID, -- References vehicles table
    
    -- Status Tracking
    status VARCHAR(50) DEFAULT 'SCHEDULED', -- SCHEDULED, DISPATCHED, EN_ROUTE, ARRIVED, COMPLETED, CANCELLED
    dispatched_at TIMESTAMP,
    arrived_at TIMESTAMP,
    completed_at TIMESTAMP,
    actual_duration_minutes INTEGER,
    
    -- Notes & Issues
    pre_delivery_notes TEXT,
    delivery_notes TEXT,
    issues_encountered TEXT,
    
    -- Confirmation
    customer_confirmed BOOLEAN DEFAULT FALSE,
    customer_confirmed_at TIMESTAMP,
    confirmation_method VARCHAR(50), -- EMAIL, PHONE, SMS
    
    -- Audit
    created_by UUID,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT unique_job_schedule UNIQUE(job_id, scheduled_date)
);

CREATE INDEX idx_schedules_date ON schedules(scheduled_date);
CREATE INDEX idx_schedules_crew ON schedules(crew_id);
CREATE INDEX idx_schedules_status ON schedules(status);
CREATE INDEX idx_schedules_route ON schedules(route_id);

-- ============================================================
-- ROUTES (Optimized delivery sequences)
-- ============================================================

CREATE TABLE routes (
    route_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    route_number VARCHAR(100) UNIQUE NOT NULL, -- e.g., 'R-2024-1120-01'
    route_date DATE NOT NULL,
    crew_id UUID,
    truck_id UUID,
    
    -- Route Planning
    total_jobs INTEGER DEFAULT 0,
    total_weight_lbs INTEGER DEFAULT 0,
    total_volume_cuft INTEGER DEFAULT 0,
    total_distance_miles DECIMAL(10, 2),
    estimated_duration_minutes INTEGER,
    
    -- Route Optimization
    optimized BOOLEAN DEFAULT FALSE,
    optimization_score DECIMAL(5, 2), -- Efficiency metric
    optimization_algorithm VARCHAR(100), -- e.g., 'NEAREST_NEIGHBOR', 'GENETIC'
    
    -- Status
    status VARCHAR(50) DEFAULT 'PLANNED', -- PLANNED, DISPATCHED, IN_PROGRESS, COMPLETED
    
    -- Timestamps
    planned_start_time TIME,
    planned_end_time TIME,
    actual_start_time TIMESTAMP,
    actual_end_time TIMESTAMP,
    
    -- Notes
    notes TEXT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_routes_date ON routes(route_date);
CREATE INDEX idx_routes_crew ON routes(crew_id);

-- ============================================================
-- CREWS & RESOURCES
-- ============================================================

CREATE TABLE crews (
    crew_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    crew_name VARCHAR(255) NOT NULL UNIQUE,
    crew_code VARCHAR(50) NOT NULL UNIQUE,
    crew_type VARCHAR(100) DEFAULT 'FRAMING', -- FRAMING, TRUSS, SPECIALTY, GENERAL
    
    -- Capacity
    max_jobs_per_day INTEGER DEFAULT 4,
    max_weight_lbs INTEGER DEFAULT 15000,
    max_volume_cuft INTEGER DEFAULT 1000,
    
    -- Skills & Certifications
    crane_certified BOOLEAN DEFAULT FALSE,
    forklift_certified BOOLEAN DEFAULT FALSE,
    specialty_skills JSONB, -- e.g., {"custom_trusses": true, "engineered_lumber": true}
    
    -- Geographic Coverage
    primary_zone VARCHAR(50),
    coverage_zones TEXT[], -- Array of zone codes
    max_drive_distance_miles INTEGER DEFAULT 50,
    
    -- Status
    active BOOLEAN DEFAULT TRUE,
    available BOOLEAN DEFAULT TRUE,
    
    -- Contact
    supervisor_name VARCHAR(255),
    supervisor_phone VARCHAR(50),
    supervisor_email VARCHAR(255),
    
    -- Notes
    notes TEXT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE crew_members (
    member_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    crew_id UUID REFERENCES crews(crew_id) ON DELETE CASCADE,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    role VARCHAR(100), -- LEAD, DRIVER, HELPER, FORKLIFT_OPERATOR
    phone VARCHAR(50),
    email VARCHAR(255),
    certifications TEXT[],
    active BOOLEAN DEFAULT TRUE,
    hired_date DATE,
    terminated_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_crew_members_crew ON crew_members(crew_id);
CREATE INDEX idx_crew_members_active ON crew_members(active);

-- ============================================================
-- VEHICLES & EQUIPMENT
-- ============================================================

CREATE TABLE vehicles (
    vehicle_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    vehicle_number VARCHAR(100) NOT NULL UNIQUE,
    vehicle_type VARCHAR(100), -- FLATBED, TRUCK, CRANE, FORKLIFT
    make VARCHAR(100),
    model VARCHAR(100),
    year INTEGER,
    license_plate VARCHAR(50),
    vin VARCHAR(50),
    
    -- Capacity
    max_weight_lbs INTEGER,
    max_volume_cuft INTEGER,
    bed_length_ft DECIMAL(5, 2),
    bed_width_ft DECIMAL(5, 2),
    
    -- Status
    status VARCHAR(50) DEFAULT 'AVAILABLE', -- AVAILABLE, IN_USE, MAINTENANCE, OUT_OF_SERVICE
    active BOOLEAN DEFAULT TRUE,
    
    -- Maintenance
    last_maintenance_date DATE,
    next_maintenance_date DATE,
    maintenance_notes TEXT,
    
    -- Assignment
    assigned_crew_id UUID REFERENCES crews(crew_id),
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_vehicles_status ON vehicles(status);
CREATE INDEX idx_vehicles_assigned_crew ON vehicles(assigned_crew_id);

-- ============================================================
-- CREW AVAILABILITY & TIME OFF
-- ============================================================

CREATE TABLE crew_availability (
    availability_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    crew_id UUID REFERENCES crews(crew_id) ON DELETE CASCADE,
    date DATE NOT NULL,
    available BOOLEAN DEFAULT TRUE,
    reason VARCHAR(100), -- SCHEDULED_OFF, VACATION, SICK, MAINTENANCE
    notes TEXT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT unique_crew_date UNIQUE(crew_id, date)
);

CREATE INDEX idx_availability_crew ON crew_availability(crew_id);
CREATE INDEX idx_availability_date ON crew_availability(date);

-- ============================================================
-- JOB HISTORY & STATUS CHANGES
-- ============================================================

CREATE TABLE job_status_history (
    history_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES jobs(job_id) ON DELETE CASCADE,
    from_status VARCHAR(50),
    to_status VARCHAR(50) NOT NULL,
    changed_by UUID, -- User ID
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    reason TEXT,
    notes TEXT
);

CREATE INDEX idx_job_history_job ON job_status_history(job_id);
CREATE INDEX idx_job_history_date ON job_status_history(changed_at);

-- ============================================================
-- DELIVERY NOTES & PHOTOS
-- ============================================================

CREATE TABLE delivery_notes (
    note_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    schedule_id UUID REFERENCES schedules(schedule_id) ON DELETE CASCADE,
    note_type VARCHAR(50), -- PRE_DELIVERY, ON_SITE, POST_DELIVERY, ISSUE, RESOLUTION
    note_text TEXT NOT NULL,
    created_by UUID,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE delivery_photos (
    photo_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    schedule_id UUID REFERENCES schedules(schedule_id) ON DELETE CASCADE,
    photo_type VARCHAR(50), -- BEFORE, AFTER, ISSUE, SITE_ACCESS
    file_path VARCHAR(500) NOT NULL,
    file_url VARCHAR(500),
    caption TEXT,
    uploaded_by UUID,
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_notes_schedule ON delivery_notes(schedule_id);
CREATE INDEX idx_photos_schedule ON delivery_photos(schedule_id);

-- ============================================================
-- GEOGRAPHIC ZONES
-- ============================================================

CREATE TABLE zones (
    zone_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    zone_code VARCHAR(50) NOT NULL UNIQUE,
    zone_name VARCHAR(255) NOT NULL,
    description TEXT,
    boundary_geojson JSONB, -- GeoJSON polygon for zone boundaries
    center_lat DECIMAL(10, 8),
    center_lon DECIMAL(11, 8),
    typical_drive_time_minutes INTEGER, -- From warehouse/yard
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ============================================================
-- MATERIALS (Optional - for MaterialFlow integration)
-- ============================================================

CREATE TABLE materials (
    material_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    material_code VARCHAR(100) NOT NULL UNIQUE,
    material_name VARCHAR(255) NOT NULL,
    category VARCHAR(100), -- LUMBER, TRUSSES, HARDWARE, FASTENERS
    unit_of_measure VARCHAR(50), -- LF, EA, BOX, BUNDLE
    weight_per_unit_lbs DECIMAL(10, 2),
    volume_per_unit_cuft DECIMAL(10, 2),
    current_price DECIMAL(10, 2),
    supplier VARCHAR(255),
    notes TEXT,
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE job_materials (
    job_material_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES jobs(job_id) ON DELETE CASCADE,
    material_id UUID REFERENCES materials(material_id),
    quantity DECIMAL(10, 2) NOT NULL,
    unit_price DECIMAL(10, 2),
    total_price DECIMAL(10, 2),
    status VARCHAR(50) DEFAULT 'PENDING', -- PENDING, ORDERED, RECEIVED, STAGED, DELIVERED
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_materials_job ON job_materials(job_id);
CREATE INDEX idx_job_materials_material ON job_materials(material_id);

-- ============================================================
-- USERS & AUTHENTICATION
-- ============================================================

CREATE TABLE users (
    user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255),
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    role VARCHAR(50) NOT NULL, -- ADMIN, SCHEDULER, CREW_LEAD, BUILDER_CONTACT, READONLY
    phone VARCHAR(50),
    active BOOLEAN DEFAULT TRUE,
    last_login TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_role ON users(role);

-- ============================================================
-- AUDIT LOG
-- ============================================================

CREATE TABLE audit_log (
    log_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(user_id),
    action VARCHAR(100) NOT NULL, -- CREATE, UPDATE, DELETE, LOGIN, LOGOUT
    entity_type VARCHAR(100), -- JOB, SCHEDULE, CREW, etc.
    entity_id UUID,
    changes JSONB, -- Old values vs new values
    ip_address VARCHAR(50),
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_audit_user ON audit_log(user_id);
CREATE INDEX idx_audit_entity ON audit_log(entity_type, entity_id);
CREATE INDEX idx_audit_created ON audit_log(created_at);

-- ============================================================
-- VIEWS FOR COMMON QUERIES
-- ============================================================

CREATE VIEW vw_pending_jobs AS
SELECT 
    j.*,
    b.name AS builder_name,
    b.short_code AS builder_code,
    c.name AS community_name,
    bp.plan_name,
    pc.config_name
FROM jobs j
LEFT JOIN builders b ON j.builder_id = b.builder_id
LEFT JOIN communities c ON j.community_id = c.community_id
LEFT JOIN building_plans bp ON j.plan_id = bp.plan_id
LEFT JOIN plan_configurations pc ON j.config_id = pc.config_id
WHERE j.status = 'PENDING'
ORDER BY j.priority_level DESC, j.requested_date ASC;

CREATE VIEW vw_scheduled_jobs AS
SELECT 
    j.*,
    s.schedule_id,
    s.scheduled_date,
    s.scheduled_time_start,
    s.scheduled_time_end,
    s.route_id,
    s.route_position,
    s.status AS schedule_status,
    cr.crew_name,
    b.name AS builder_name
FROM jobs j
INNER JOIN schedules s ON j.job_id = s.job_id
LEFT JOIN crews cr ON s.crew_id = cr.crew_id
LEFT JOIN builders b ON j.builder_id = b.builder_id
WHERE s.status IN ('SCHEDULED', 'DISPATCHED', 'EN_ROUTE')
ORDER BY s.scheduled_date ASC, s.route_position ASC;

CREATE VIEW vw_crew_utilization AS
SELECT 
    c.crew_id,
    c.crew_name,
    COUNT(s.schedule_id) AS jobs_scheduled,
    SUM(j.estimated_weight_lbs) AS total_weight,
    SUM(j.estimated_volume_cuft) AS total_volume,
    c.max_jobs_per_day,
    c.max_weight_lbs,
    c.max_volume_cuft,
    ROUND(100.0 * COUNT(s.schedule_id) / NULLIF(c.max_jobs_per_day, 0), 2) AS job_capacity_pct,
    ROUND(100.0 * SUM(j.estimated_weight_lbs) / NULLIF(c.max_weight_lbs, 0), 2) AS weight_capacity_pct
FROM crews c
LEFT JOIN schedules s ON c.crew_id = s.crew_id AND s.scheduled_date = CURRENT_DATE
LEFT JOIN jobs j ON s.job_id = j.job_id
WHERE c.active = TRUE
GROUP BY c.crew_id, c.crew_name, c.max_jobs_per_day, c.max_weight_lbs, c.max_volume_cuft;
```

---

## **C) API Endpoints Design**

### **API Architecture**

**Base URL:** `https://api.mindflow.app/v1`

**Authentication:** JWT Bearer tokens

**Response Format:** JSON

**Error Handling:** Standard HTTP status codes + structured error messages

---

### **API Endpoint Specification**

#### **1. AUTHENTICATION**
```
POST /auth/login
Body: { email, password }
Response: { token, user_id, role, expires_at }

POST /auth/logout
Headers: Authorization: Bearer {token}
Response: { success: true }

POST /auth/refresh
Headers: Authorization: Bearer {token}
Response: { token, expires_at }

GET /auth/me
Headers: Authorization: Bearer {token}
Response: { user_id, email, first_name, last_name, role }
```

---

#### **2. BUILDERS**
```
GET /builders
Query: ?active=true&limit=50&offset=0
Response: { builders: [], total: 0, page: 1 }

GET /builders/{builder_id}
Response: { builder_id, name, short_code, priority_level, ... }

POST /builders
Body: { name, short_code, contact_email, ... }
Response: { builder_id, ... }

PUT /builders/{builder_id}
Body: { name, contact_email, ... }
Response: { builder_id, ... }

DELETE /builders/{builder_id}
Response: { success: true }

GET /builders/{builder_id}/communities
Response: { communities: [] }

GET /builders/{builder_id}/jobs
Query: ?status=PENDING&limit=50
Response: { jobs: [], total: 0 }
```

---

#### **3. JOBS**
```
GET /jobs
Query: ?status=PENDING&builder_id={uuid}&zone=PDX-WEST&date_from=2024-11-01&date_to=2024-11-30&limit=50&offset=0
Response: {
  jobs: [
    {
      job_id, job_number, builder_name, street_address,
      status, priority_level, requested_date, estimated_weight_lbs,
      estimated_volume_cuft, zone_code, ...
    }
  ],
  total: 150,
  page: 1,
  pages: 3
}

GET /jobs/{job_id}
Response: {
  job_id, job_number, builder: {}, community: {},
  plan: {}, config: {}, schedule: {},
  materials: [], status_history: []
}

POST /jobs
Body: {
  builder_id, plan_id, config_id,
  street_address, city, state, zip,
  job_type, priority_level,
  requested_date, requested_time_window,
  customer_name, customer_phone,
  special_requirements, ...
}
Response: { job_id, job_number, ... }

PUT /jobs/{job_id}
Body: { status, priority_level, ... }
Response: { job_id, ... }

PATCH /jobs/{job_id}/status
Body: { status: "SCHEDULED", reason: "..." }
Response: { job_id, status, updated_at }

DELETE /jobs/{job_id}
Response: { success: true }

POST /jobs/{job_id}/notes
Body: { note_text, note_type }
Response: { note_id, ... }

POST /jobs/{job_id}/photos
Body: multipart/form-data { photo: file, photo_type, caption }
Response: { photo_id, file_url, ... }

GET /jobs/{job_id}/history
Response: { history: [ { from_status, to_status, changed_at, changed_by, reason } ] }
```

---

#### **4. SCHEDULES**
```
GET /schedules
Query: ?date=2024-11-20&crew_id={uuid}&status=SCHEDULED
Response: {
  schedules: [
    {
      schedule_id, job: {}, scheduled_date,
      scheduled_time_start, crew: {}, route_id,
      route_position, status, ...
    }
  ],
  total: 25
}

GET /schedules/{schedule_id}
Response: {
  schedule_id, job: {}, crew: {}, vehicle: {},
  scheduled_date, scheduled_time_start, route: {},
  notes: [], photos: []
}

POST /schedules
Body: {
  job_id, scheduled_date,
  scheduled_time_start, scheduled_time_end,
  crew_id, truck_id, route_id,
  pre_delivery_notes, ...
}
Response: { schedule_id, ... }

PUT /schedules/{schedule_id}
Body: { scheduled_date, crew_id, ... }
Response: { schedule_id, ... }

PATCH /schedules/{schedule_id}/status
Body: { status: "DISPATCHED", notes: "..." }
Response: { schedule_id, status, updated_at }

DELETE /schedules/{schedule_id}
Response: { success: true }

POST /schedules/batch
Body: {
  jobs: [ job_id1, job_id2, ... ],
  scheduled_date, crew_id, ...
}
Response: { schedules: [ { schedule_id, job_id, ... } ] }

POST /schedules/{schedule_id}/confirm
Body: { confirmation_method: "EMAIL" }
Response: { customer_confirmed: true, customer_confirmed_at }
```

---

#### **5. CREWS**
```
GET /crews
Query: ?active=true&available=true&zone=PDX-WEST
Response: {
  crews: [
    {
      crew_id, crew_name, crew_type,
      max_jobs_per_day, max_weight_lbs,
      primary_zone, active, available,
      current_utilization: { jobs: 2, weight_pct: 45.5 }
    }
  ]
}

GET /crews/{crew_id}
Response: {
  crew_id, crew_name, members: [],
  assigned_vehicle: {}, availability: {},
  current_schedule: []
}

POST /crews
Body: {
  crew_name, crew_code, crew_type,
  max_jobs_per_day, primary_zone, ...
}
Response: { crew_id, ... }

PUT /crews/{crew_id}
Body: { max_jobs_per_day, available, ... }
Response: { crew_id, ... }

DELETE /crews/{crew_id}
Response: { success: true }

GET /crews/{crew_id}/schedule
Query: ?date_from=2024-11-01&date_to=2024-11-30
Response: {
  schedule: [
    { date, jobs: [ { schedule_id, job: {}, time_start } ] }
  ]
}

GET /crews/{crew_id}/availability
Query: ?date_from=2024-11-01&date_to=2024-11-30
Response: {
  availability: [
    { date, available, reason }
  ]
}

POST /crews/{crew_id}/availability
Body: { date, available, reason }
Response: { availability_id, ... }

GET /crews/{crew_id}/utilization
Query: ?date=2024-11-20
Response: {
  jobs_scheduled: 3,
  jobs_capacity: 4,
  weight_used: 8500,
  weight_capacity: 15000,
  utilization_pct: 75.0
}
```

---

#### **6. ROUTES**
```
GET /routes
Query: ?date=2024-11-20&crew_id={uuid}
Response: {
  routes: [
    {
      route_id, route_number, route_date,
      crew: {}, total_jobs, total_distance_miles,
      status, optimized, ...
    }
  ]
}

GET /routes/{route_id}
Response: {
  route_id, route_number, crew: {},
  jobs: [
    {
      position, schedule: {}, job: {},
      estimated_arrival_time, distance_from_previous
    }
  ],
  total_distance_miles, estimated_duration_minutes,
  optimization_score
}

POST /routes
Body: {
  route_date, crew_id,
  schedule_ids: [ schedule_id1, schedule_id2, ... ]
}
Response: { route_id, route_number, ... }

POST /routes/{route_id}/optimize
Body: {
  algorithm: "NEAREST_NEIGHBOR", // or "GENETIC"
  start_location: { lat, lon }
}
Response: {
  route_id, optimized: true,
  jobs: [ { position, schedule_id, estimated_arrival } ],
  total_distance_miles, optimization_score
}

PUT /routes/{route_id}/reorder
Body: {
  schedule_positions: [
    { schedule_id, position: 1 },
    { schedule_id, position: 2 }
  ]
}
Response: { route_id, jobs: [] }

PATCH /routes/{route_id}/status
Body: { status: "DISPATCHED" }
Response: { route_id, status, actual_start_time }

DELETE /routes/{route_id}
Response: { success: true }
```

---

#### **7. SCHEDULING INTELLIGENCE**
```
POST /scheduling/suggest-crew
Body: {
  job_id,
  requested_date, // optional
  zone_code // optional if job already has GPS
}
Response: {
  suggested_crews: [
    {
      crew_id, crew_name,
      match_score: 95.5,
      reasons: ["Same zone", "Capacity available", "Specialized skills"],
      availability: true,
      current_load: { jobs: 2, weight_pct: 45.0 }
    }
  ]
}

POST /scheduling/suggest-date
Body: {
  job_id,
  crew_id, // optional
  date_range_start,
  date_range_end
}
Response: {
  suggested_dates: [
    {
      date, available_crews: [],
      utilization_score: 65.0,
      reasons: ["Low crew utilization", "Good weather forecast"]
    }
  ]
}

POST /scheduling/auto-schedule
Body: {
  job_ids: [ job_id1, job_id2, ... ],
  date_range_start,
  date_range_end,
  constraints: {
    prefer_same_crew_per_builder: true,
    max_jobs_per_route: 4,
    optimize_by: "distance" // or "time" or "cost"
  }
}
Response: {
  scheduled: [
    { job_id, schedule_id, crew_id, scheduled_date }
  ],
  unscheduled: [
    { job_id, reason: "No crew capacity available" }
  ],
  summary: {
    total_jobs: 10,
    scheduled_count: 8,
    routes_created: 3
  }
}

GET /scheduling/capacity
Query: ?date_from=2024-11-20&date_to=2024-11-30
Response: {
  capacity_by_date: [
    {
      date,
      total_crew_capacity: 16, // jobs
      scheduled_jobs: 12,
      available_capacity: 4,
      utilization_pct: 75.0,
      crews: [
        { crew_id, crew_name, jobs_scheduled: 3, capacity: 4 }
      ]
    }
  ]
}

GET /scheduling/conflicts
Query: ?date=2024-11-20
Response: {
  conflicts: [
    {
      type: "OVER_CAPACITY",
      crew_id, crew_name,
      scheduled_jobs: 5,
      max_capacity: 4,
      affected_schedules: [ schedule_id1, schedule_id2 ]
    },
    {
      type: "TIME_OVERLAP",
      crew_id,
      overlapping_schedules: [ schedule_id1, schedule_id2 ]
    }
  ]
}
```

---

#### **8. ANALYTICS & REPORTING**
```
GET /analytics/dashboard
Query: ?date_from=2024-11-01&date_to=2024-11-30
Response: {
  summary: {
    total_jobs: 150,
    completed_jobs: 120,
    on_time_delivery_rate: 94.5,
    average_job_duration_minutes: 235,
    total_revenue: 125000.00
  },
  by_builder: [
    { builder_name, job_count, revenue, on_time_rate }
  ],
  by_crew: [
    { crew_name, jobs_completed, utilization_rate, avg_duration }
  ]
}

GET /analytics/crew-performance
Query: ?crew_id={uuid}&date_from=2024-11-01&date_to=2024-11-30
Response: {
  crew_name,
  jobs_completed: 45,
  on_time_deliveries: 43,
  on_time_rate: 95.6,
  average_duration_minutes: 215,
  total_distance_miles: 850,
  issues_reported: 3
}

GET /analytics/builder-report
Query: ?builder_id={uuid}&date_from=2024-11-01&date_to=2024-11-30
Response: {
  builder_name,
  jobs_total: 60,
  jobs_completed: 55,
  jobs_pending: 5,
  on_time_rate: 96.4,
  average_lead_time_days: 4.2,
  top_plans: [
    { plan_name, job_count, avg_duration }
  ]
}

GET /analytics/zone-heatmap
Query: ?date_from=2024-11-01&date_to=2024-11-30
Response: {
  zones: [
    {
      zone_code, zone_name,
      job_count, total_distance_miles,
      avg_duration_minutes,
      center_lat, center_lon
    }
  ]
}
```

---

#### **9. RICHMOND AMERICAN BAT MIGRATION**
```
POST /migration/bat/validate
Body: multipart/form-data { excel_file: file }
Response: {
  valid: true,
  plans_found: 61,
  validation_errors: [],
  preview: [
    { plan_name, walls_count, roof_trusses, sq_ft }
  ]
}

POST /migration/bat/import
Body: multipart/form-data { 
  excel_file: file,
  builder_id: uuid,
  dry_run: true // Set to false for actual import
}
Response: {
  success: true,
  plans_imported: 61,
  jobs_created: 61,
  errors: [],
  summary: {
    total_plans: 61,
    total_walls: 2450,
    total_trusses: 1850,
    total_estimated_weight: 825000
  }
}

GET /migration/bat/status/{import_id}
Response: {
  import_id, status: "IN_PROGRESS", // or "COMPLETED"
  progress_pct: 75.0,
  plans_processed: 45,
  plans_total: 61
}

POST /migration/bat/rollback/{import_id}
Response: {
  success: true,
  jobs_deleted: 61,
  plans_deleted: 61
}
```

---

#### **10. EXTERNAL INTEGRATIONS**
```
POST /integrations/materialflow/sync
Body: { po_number }
Response: {
  success: true,
  materials_synced: 25,
  jobs_updated: 5
}

POST /integrations/sharepoint/upload
Body: {
  job_id,
  document_type: "TAKEOFF", // or "INVOICE", "DELIVERY_RECEIPT"
  file_url
}
Response: { success: true, sharepoint_url }

POST /integrations/excel/export
Body: {
  job_ids: [ job_id1, job_id2, ... ],
  format: "BAT" // or "RF", "MATERIALFLOW"
}
Response: {
  file_url: "https://...",
  expires_at
}

GET /integrations/randomlengths/prices
Query: ?date=2024-11-20
Response: {
  prices: [
    { material_code, price, date, source: "Random Lengths" }
  ]
}
```

---

#### **11. WEBHOOKS (Optional)**
```
POST /webhooks/register
Body: {
  event_type: "job.created", // or "schedule.completed"
  url: "https://your-app.com/webhook",
  secret
}
Response: { webhook_id, ... }

GET /webhooks
Response: { webhooks: [] }

DELETE /webhooks/{webhook_id}
Response: { success: true }

# Webhook Payload Example:
POST https://your-app.com/webhook
Headers: X-Signature: {hmac_signature}
Body: {
  event: "job.created",
  timestamp: "2024-11-20T10:30:00Z",
  data: { job_id, job_number, ... }
}

API Error Responses
json// 400 Bad Request
{
  "error": "VALIDATION_ERROR",
  "message": "Invalid request parameters",
  "details": {
    "scheduled_date": "Date must be in the future",
    "crew_id": "Crew does not exist"
  }
}

// 401 Unauthorized
{
  "error": "UNAUTHORIZED",
  "message": "Invalid or expired token"
}

// 403 Forbidden
{
  "error": "FORBIDDEN",
  "message": "Insufficient permissions for this operation"
}

// 404 Not Found
{
  "error": "NOT_FOUND",
  "message": "Job with ID {job_id} not found"
}

// 409 Conflict
{
  "error": "CONFLICT",
  "message": "Crew already scheduled at this time",
  "details": {
    "existing_schedule_id": "uuid",
    "conflict_type": "TIME_OVERLAP"
  }
}

// 500 Internal Server Error
{
  "error": "INTERNAL_ERROR",
  "message": "An unexpected error occurred",
  "request_id": "req_abc123"
}
```

---

## **D) Richmond American BAT Migration Workflow**

### **Migration Overview**

**Objective:** Migrate 61 legacy Excel-based building plans from Richmond American's current BAT system into the new MindFlow Scheduling Module with zero data loss and minimal disruption.

**Timeline:** 5 weeks (with 1 week buffer)

**Team Required:**
- 1 Technical Lead (you)
- 1 Data Analyst (validation)
- 1 Scheduler (UAT & training)
- Richmond American liaison (as needed)

---

### **Pre-Migration Checklist**
```
□ Complete inventory of all 61 Excel files
□ Document current BAT workflow
□ Identify data owners at Richmond American
□ Set up development/staging environment
□ Create data backup strategy
□ Define rollback plan
□ Schedule training sessions

Phase 1: Discovery & Data Extraction (Week 1)
Day 1-2: Inventory & Analysis
python# Script: inventory_bat_files.py

import os
import pandas as pd
from pathlib import Path

def inventory_bat_files(directory):
    """Scan directory for BAT Excel files and extract metadata"""
    
    inventory = []
    
    for file in Path(directory).rglob("*.xlsx"):
        try:
            # Read first sheet to get plan metadata
            df = pd.read_excel(file, sheet_name=0, nrows=10)
            
            metadata = {
                'file_path': str(file),
                'file_name': file.name,
                'file_size_kb': file.stat().st_size / 1024,
                'modified_date': file.stat().st_mtime,
                'sheets': pd.ExcelFile(file).sheet_names,
                'has_walls': 'Walls' in pd.ExcelFile(file).sheet_names,
                'has_roof': 'Roof' in pd.ExcelFile(file).sheet_names,
                'has_materials': 'Materials' in pd.ExcelFile(file).sheet_names
            }
            
            # Try to extract plan name
            if 'Plan Name' in df.columns:
                metadata['plan_name'] = df['Plan Name'].iloc[0]
            elif 'plan_name' in df.columns:
                metadata['plan_name'] = df['plan_name'].iloc[0]
            else:
                metadata['plan_name'] = file.stem  # Use filename
            
            inventory.append(metadata)
            
        except Exception as e:
            print(f"Error processing {file}: {e}")
            inventory.append({
                'file_path': str(file),
                'file_name': file.name,
                'error': str(e)
            })
    
    return pd.DataFrame(inventory)

# Run inventory
inventory_df = inventory_bat_files("/path/to/richmond_american/bat_files")
inventory_df.to_csv("bat_inventory.csv", index=False)

print(f"Found {len(inventory_df)} BAT files")
print(f"Plans with Walls data: {inventory_df['has_walls'].sum()}")
print(f"Plans with Roof data: {inventory_df['has_roof'].sum()}")
print(f"Files with errors: {inventory_df['error'].notna().sum()}")
Deliverables:

bat_inventory.csv - Complete file inventory
Data quality report
List of files needing remediation


Day 3-5: Data Extraction & Standardization
python# Script: extract_bat_data.py

import pandas as pd
import json
from typing import Dict, List

class BATExtractor:
    """Extract and standardize data from Richmond American BAT Excel files"""
    
    def __init__(self, file_path):
        self.file_path = file_path
        self.excel_file = pd.ExcelFile(file_path)
        self.data = {}
    
    def extract_plan_info(self) -> Dict:
        """Extract basic plan information from Summary sheet"""
        try:
            df = pd.read_excel(self.file_path, sheet_name='Summary')
            
            return {
                'plan_name': self._safe_get(df, 'Plan Name', 0),
                'plan_code': self._safe_get(df, 'Plan Code', 0),
                'square_footage': self._safe_get(df, 'Square Footage', 0, int),
                'stories': self._safe_get(df, 'Stories', 0, float),
                'bedrooms': self._safe_get(df, 'Bedrooms', 0, int),
                'bathrooms': self._safe_get(df, 'Bathrooms', 0, float),
                'garage_bays': self._safe_get(df, 'Garage Bays', 0, int),
                'description': self._safe_get(df, 'Description', 0)
            }
        except Exception as e:
            print(f"Error extracting plan info from {self.file_path}: {e}")
            return {}
    
    def extract_walls(self) -> List[Dict]:
        """Extract wall takeoff data"""
        try:
            df = pd.read_excel(self.file_path, sheet_name='Walls')
            
            walls = []
            for _, row in df.iterrows():
                walls.append({
                    'wall_id': row.get('Wall ID'),
                    'wall_type': row.get('Type'),
                    'length_ft': float(row.get('Length', 0)),
                    'height_ft': float(row.get('Height', 8)),  # Default 8ft
                    'studs_count': int(row.get('Studs', 0)),
                    'plates_lf': float(row.get('Plates LF', 0)),
                    'sheathing_sf': float(row.get('Sheathing SF', 0)),
                    'notes': row.get('Notes', '')
                })
            
            return walls
        except Exception as e:
            print(f"Error extracting walls from {self.file_path}: {e}")
            return []
    
    def extract_roof(self) -> Dict:
        """Extract roof/truss data"""
        try:
            df = pd.read_excel(self.file_path, sheet_name='Roof')
            
            return {
                'truss_count': self._safe_sum(df, 'Truss Count'),
                'truss_types': self._safe_list(df, 'Truss Type'),
                'total_lf': self._safe_sum(df, 'Total LF'),
                'sheathing_sf': self._safe_sum(df, 'Sheathing SF'),
                'estimated_weight_lbs': self._safe_sum(df, 'Weight (lbs)'),
                'notes': self._safe_get(df, 'Notes', 0)
            }
        except Exception as e:
            print(f"Error extracting roof from {self.file_path}: {e}")
            return {}
    
    def extract_materials(self) -> List[Dict]:
        """Extract material list"""
        try:
            df = pd.read_excel(self.file_path, sheet_name='Materials')
            
            materials = []
            for _, row in df.iterrows():
                materials.append({
                    'material_code': row.get('Code'),
                    'material_name': row.get('Description'),
                    'quantity': float(row.get('Quantity', 0)),
                    'unit': row.get('Unit'),
                    'unit_price': float(row.get('Unit Price', 0)),
                    'total_price': float(row.get('Total', 0)),
                    'supplier': row.get('Supplier', '')
                })
            
            return materials
        except Exception as e:
            print(f"Error extracting materials from {self.file_path}: {e}")
            return []
    
    def calculate_estimates(self) -> Dict:
        """Calculate estimated capacity requirements"""
        walls = self.extract_walls()
        roof = self.extract_roof()
        
        total_lf = sum(w['length_ft'] for w in walls)
        total_studs = sum(w['studs_count'] for w in walls)
        total_trusses = roof.get('truss_count', 0)
        
        # Rough weight estimates (industry averages)
        stud_weight_lbs = 8  # 2x4x8 stud
        truss_weight_lbs = 45  # Average truss weight
        sheathing_weight_lbs_per_sf = 2.5
        
        total_weight = (
            total_studs * stud_weight_lbs +
            total_trusses * truss_weight_lbs +
            roof.get('sheathing_sf', 0) * sheathing_weight_lbs_per_sf
        )
        
        # Rough volume estimates (cubic feet)
        volume = total_lf * 0.5  # Rough approximation
        
        # Duration estimate (minutes)
        # Rule of thumb: 3 minutes per LF for complete framing
        duration = int(total_lf * 3)
        
        return {
            'estimated_weight_lbs': int(total_weight),
            'estimated_volume_cuft': int(volume),
            'estimated_duration_minutes': duration,
            'total_linear_feet': int(total_lf),
            'total_walls': len(walls),
            'total_studs': total_studs,
            'total_trusses': total_trusses
        }
    
    def extract_all(self) -> Dict:
        """Extract all data from BAT file"""
        self.data = {
            'plan_info': self.extract_plan_info(),
            'walls': self.extract_walls(),
            'roof': self.extract_roof(),
            'materials': self.extract_materials(),
            'estimates': self.calculate_estimates()
        }
        
        return self.data
    
    @staticmethod
    def _safe_get(df, column, index, dtype=str):
        """Safely get value from dataframe"""
        try:
            if column in df.columns:
                val = df[column].iloc[index]
                return dtype(val) if pd.notna(val) else None
            return None
        except:
            return None
    
    @staticmethod
    def _safe_sum(df, column):
        """Safely sum column values"""
        try:
            if column in df.columns:
                return float(df[column].sum())
            return 0
        except:
            return 0
    
    @staticmethod
    def _safe_list(df, column):
        """Safely get unique list of values"""
        try:
            if column in df.columns:
                return df[column].dropna().unique().tolist()
            return []
        except:
            return []

# Process all files
def process_all_bat_files(inventory_csv, output_dir):
    """Process all BAT files and export to JSON"""
    
    inventory = pd.read_csv(inventory_csv)
    results = []
    errors = []
    
    for idx, row in inventory.iterrows():
        file_path = row['file_path']
        print(f"Processing {idx + 1}/{len(inventory)}: {row['file_name']}")
        
        try:
            extractor = BATExtractor(file_path)
            data = extractor.extract_all()
            data['source_file'] = row['file_name']
            
            # Save individual file
            output_file = f"{output_dir}/{row['file_name'].replace('.xlsx', '.json')}"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2)
            
            results.append({
                'file_name': row['file_name'],
                'status': 'SUCCESS',
                'plan_name': data['plan_info'].get('plan_name'),
                'walls_count': len(data['walls']),
                'trusses_count': data['roof'].get('truss_count', 0),
                'output_file': output_file
            })
            
        except Exception as e:
            print(f"ERROR: {e}")
            errors.append({
                'file_name': row['file_name'],
                'status': 'ERROR',
                'error': str(e)
            })
    
    # Save results summary
    pd.DataFrame(results + errors).to_csv(f"{output_dir}/extraction_results.csv", index=False)
    
    print(f"\nCompleted: {len(results)} success, {len(errors)} errors")
    return results, errors

# Run extraction
results, errors = process_all_bat_files(
    "bat_inventory.csv",
    "/path/to/output/json"
)
Deliverables:

61 JSON files with standardized data
extraction_results.csv - Processing summary
Data quality report with anomalies


Phase 2: Data Validation & Transformation (Week 2)
Data Validation Rules
python# Script: validate_bat_data.py

import json
from pathlib import Path
from typing import List, Dict

class BATValidator:
    """Validate extracted BAT data before import"""
    
    def __init__(self, json_dir):
        self.json_dir = json_dir
        self.validation_results = []
    
    def validate_all(self) -> List[Dict]:
        """Validate all JSON files"""
        
        for json_file in Path(self.json_dir).glob("*.json"):
            with open(json_file) as f:
                data = json.load(f)
            
            result = {
                'file': json_file.name,
                'valid': True,
                'errors': [],
                'warnings': []
            }
            
            # Required fields validation
            plan_info = data.get('plan_info', {})
            if not plan_info.get('plan_name'):
                result['errors'].append("Missing plan_name")
                result['valid'] = False
            
            # Reasonable value checks
            sq_ft = plan_info.get('square_footage', 0)
            if sq_ft < 500 or sq_ft > 10000:
                result['warnings'].append(f"Unusual square footage: {sq_ft}")
            
            walls = data.get('walls', [])
            if len(walls) < 4:
                result['warnings'].append(f"Only {len(walls)} walls found (expected more)")
            
            roof = data.get('roof', {})
            trusses = roof.get('truss_count', 0)
            if trusses == 0:
                result['warnings'].append("No trusses found")
            
            estimates = data.get('estimates', {})
            weight = estimates.get('estimated_weight_lbs', 0)
            if weight > 50000:
                result['warnings'].append(f"Very high weight estimate: {weight} lbs")
            
            # Data consistency checks
            materials = data.get('materials', [])
            if len(materials) == 0:
                result['warnings'].append("No materials list found")
            
            total_lf = estimates.get('total_linear_feet', 0)
            if total_lf < 100:
                result['warnings'].append(f"Low total LF: {total_lf}")
            
            self.validation_results.append(result)
        
        return self.validation_results
    
    def generate_report(self, output_file):
        """Generate validation report"""
        import pandas as pd
        
        df = pd.DataFrame(self.validation_results)
        
        print("\n=== VALIDATION REPORT ===")
        print(f"Total files: {len(df)}")
        print(f"Valid: {df['valid'].sum()}")
        print(f"Invalid: {(~df['valid']).sum()}")
        print(f"Files with warnings: {df['warnings'].apply(len).gt(0).sum()}")
        
        # Save detailed report
        df.to_csv(output_file, index=False)
        
        # Print errors
        if (~df['valid']).any():
            print("\n=== FILES WITH ERRORS ===")
            for _, row in df[~df['valid']].iterrows():
                print(f"{row['file']}: {row['errors']}")
        
        return df

# Run validation
validator = BATValidator("/path/to/output/json")
results = validator.validate_all()
report = validator.generate_report("validation_report.csv")
Deliverables:

validation_report.csv
List of files requiring manual review
Data remediation plan


Data Transformation for API
python# Script: transform_for_import.py

import json
import uuid
from datetime import datetime
from pathlib import Path

class BATTransformer:
    """Transform validated BAT data into MindFlow API format"""
    
    def __init__(self, json_dir, builder_id, default_community_id=None):
        self.json_dir = json_dir
        self.builder_id = builder_id
        self.default_community_id = default_community_id
    
    def transform_file(self, json_file) -> Dict:
        """Transform single BAT file to API-ready format"""
        
        with open(json_file) as f:
            data = json.load(f)
        
        plan_info = data.get('plan_info', {})
        estimates = data.get('estimates', {})
        roof = data.get('roof', {})
        walls = data.get('walls', [])
        
        # Building Plan
        building_plan = {
            'builder_id': self.builder_id,
            'plan_name': plan_info.get('plan_name'),
            'plan_code': plan_info.get('plan_code'),
            'square_footage': plan_info.get('square_footage'),
            'stories': plan_info.get('stories'),
            'bedrooms': plan_info.get('bedrooms'),
            'bathrooms': plan_info.get('bathrooms'),
            'garage_bays': plan_info.get('garage_bays'),
            'description': plan_info.get('description')
        }
        
        # Plan Configuration (Standard config)
        plan_config = {
            'config_name': 'Standard',
            'walls_count': len(walls),
            'roof_trusses_count': roof.get('truss_count'),
            'estimated_linear_feet': estimates.get('total_linear_feet'),
            'estimated_weight_lbs': estimates.get('estimated_weight_lbs'),
            'estimated_volume_cuft': estimates.get('estimated_volume_cuft'),
            'estimated_duration_minutes': estimates.get('estimated_duration_minutes'),
            'material_list_json': {
                'walls': walls,
                'roof': roof,
                'materials': data.get('materials', [])
            }
        }
        
        # Job (Template - one per plan)
        job = {
            'job_number': f"RHAM-{plan_info.get('plan_code', 'UNKNOWN')}-TEMPLATE",
            'builder_id': self.builder_id,
            'community_id': self.default_community_id,
            'job_type': 'COMPLETE_FRAMING',
            'priority_level': 'STANDARD',
            'status': 'TEMPLATE',  # Special status for template jobs
            'estimated_weight_lbs': estimates.get('estimated_weight_lbs'),
            'estimated_volume_cuft': estimates.get('estimated_volume_cuft'),
            'estimated_duration_minutes': estimates.get('estimated_duration_minutes'),
            'requires_crane': False,
            'requires_forklift': True,
            'bat_order_id': plan_info.get('plan_code'),
            'external_reference': data.get('source_file')
        }
        
        return {
            'source_file': data.get('source_file'),
            'building_plan': building_plan,
            'plan_config': plan_config,
            'job_template': job
        }
    
    def transform_all(self, output_file='import_batch.json'):
        """Transform all files and create import batch"""
        
        import_batch = {
            'batch_id': str(uuid.uuid4()),
            'created_at': datetime.now().isoformat(),
            'builder_id': self.builder_id,
            'plans': []
        }
        
        for json_file in Path(self.json_dir).glob("*.json"):
            try:
                transformed = self.transform_file(json_file)
                import_batch['plans'].append(transformed)
                print(f"Transformed: {json_file.name}")
            except Exception as e:
                print(f"ERROR transforming {json_file.name}: {e}")
        
        # Save batch file
        with open(output_file, 'w') as f:
            json.dump(import_batch, f, indent=2)
        
        print(f"\nCreated import batch with {len(import_batch['plans'])} plans")
        print(f"Saved to: {output_file}")
        
        return import_batch

# Run transformation
transformer = BATTransformer(
    json_dir="/path/to/output/json",
    builder_id="richmond-american-uuid",
    default_community_id="sunset-ridge-uuid"  # Optional
)

batch = transformer.transform_all("richmond_american_import_batch.json")
Deliverables:

richmond_american_import_batch.json - Ready for API import
Transformation summary report


Phase 3: System Import & Testing (Week 3)
Import Script
python# Script: import_to_mindflow.py

import requests
import json
from typing import Dict, List
import time

class MindFlowImporter:
    """Import BAT data into MindFlow via API"""
    
    def __init__(self, api_base_url, api_token):
        self.api_base_url = api_base_url.rstrip('/')
        self.api_token = api_token
        self.headers = {
            'Authorization': f'Bearer {api_token}',
            'Content-Type': 'application/json'
        }
        self.results = {
            'plans_created': [],
            'configs_created': [],
            'jobs_created': [],
            'errors': []
        }
    
    def import_batch(self, batch_file, dry_run=True):
        """Import entire batch"""
        
        with open(batch_file) as f:
            batch = json.load(f)
        
        print(f"Starting import of {len(batch['plans'])} plans")
        print(f"Dry run: {dry_run}")
        
        for idx, plan_data in enumerate(batch['plans'], 1):
            print(f"\n[{idx}/{len(batch['plans'])}] Processing: {plan_data['source_file']}")
            
            try:
                # 1. Create Building Plan
                plan_response = self._create_plan(
                    plan_data['building_plan'],
                    dry_run
                )
                
                if not dry_run and plan_response:
                    plan_id = plan_response['plan_id']
                    self.results['plans_created'].append(plan_id)
                    
                    # 2. Create Plan Configuration
                    config_data = plan_data['plan_config']
                    config_data['plan_id'] = plan_id
                    
                    config_response = self._create_config(
                        config_data,
                        dry_run
                    )
                    
                    if config_response:
                        config_id = config_response['config_id']
                        self.results['configs_created'].append(config_id)
                        
                        # 3. Create Job Template
                        job_data = plan_data['job_template']
                        job_data['plan_id'] = plan_id
                        job_data['config_id'] = config_id
                        
                        job_response = self._create_job(
                            job_data,
                            dry_run
                        )
                        
                        if job_response:
                            self.results['jobs_created'].append(
                                job_response['job_id']
                            )
                
                # Rate limiting
                time.sleep(0.5)
                
            except Exception as e:
                error = {
                    'file': plan_data['source_file'],
                    'error': str(e)
                }
                self.results['errors'].append(error)
                print(f"ERROR: {e}")
        
        self._print_summary()
        return self.results
    
    def _create_plan(self, plan_data, dry_run):
        """Create building plan via API"""
        
        if dry_run:
            print(f"  [DRY RUN] Would create plan: {plan_data.get('plan_name')}")
            return {'plan_id': 'dry-run-id'}
        
        response = requests.post(
            f"{self.api_base_url}/plans",
            headers=self.headers,
            json=plan_data
        )
        
        if response.status_code == 201:
            print(f"  ✓ Created plan: {plan_data.get('plan_name')}")
            return response.json()
        else:
            raise Exception(f"Plan creation failed: {response.text}")
    
    def _create_config(self, config_data, dry_run):
        """Create plan configuration via API"""
        
        if dry_run:
            print(f"  [DRY RUN] Would create config: {config_data.get('config_name')}")
            return {'config_id': 'dry-run-id'}
        
        response = requests.post(
            f"{self.api_base_url}/plan-configurations",
            headers=self.headers,
            json=config_data
        )
        
        if response.status_code == 201:
            print(f"  ✓ Created config: {config_data.get('config_name')}")
            return response.json()
        else:
            raise Exception(f"Config creation failed: {response.text}")
    
    def _create_job(self, job_data, dry_run):
        """Create job template via API"""
        
        if dry_run:
            print(f"  [DRY RUN] Would create job: {job_data.get('job_number')}")
            return {'job_id': 'dry-run-id'}
        
        response = requests.post(
            f"{self.api_base_url}/jobs",
            headers=self.headers,
            json=job_data
        )
        
        if response.status_code == 201:
            print(f"  ✓ Created job: {job_data.get('job_number')}")
            return response.json()
        else:
            raise Exception(f"Job creation failed: {response.text}")
    
    def _print_summary(self):
        """Print import summary"""
        
        print("\n" + "="*50)
        print("IMPORT SUMMARY")
        print("="*50)
        print(f"Plans created: {len(self.results['plans_created'])}")
        print(f"Configs created: {len(self.results['configs_created'])}")
        print(f"Jobs created: {len(self.results['jobs_created'])}")
        print(f"Errors: {len(self.results['errors'])}")
        
        if self.results['errors']:
            print("\nERRORS:")
            for err in self.results['errors']:
                print(f"  - {err['file']}: {err['error']}")

# Run import
importer = MindFlowImporter(
    api_base_url="https://api.mindflow.app/v1",
    api_token="your-api-token-here"
)

# DRY RUN first
results_dry = importer.import_batch(
    "richmond_american_import_batch.json",
    dry_run=True
)

# If dry run looks good, run for real
# results_live = importer.import_batch(
#     "richmond_american_import_batch.json",
#     dry_run=False
# )
```

**Testing Checklist:**
```
□ Dry run completed successfully
□ Sample data spot-checked in database
□ All 61 plans imported
□ Wall counts match source data
□ Truss counts match source data
□ Weight estimates reasonable
□ Volume estimates reasonable
□ Duration estimates reasonable
□ Material lists preserved
□ Plan codes unique
□ No duplicate data

Phase 4: Parallel Operation & Validation (Week 4)
Comparison Script
python# Script: compare_old_vs_new.py

import pandas as pd

def compare_systems(old_excel_dir, new_api_url, api_token):
    """Compare legacy Excel vs new MindFlow system"""
    
    # Extract from old system
    old_data = extract_from_excel_files(old_excel_dir)
    
    # Fetch from new system
    new_data = fetch_from_api(new_api_url, api_token)
    
    # Compare
    comparison = []
    
    for plan_name in old_data['plan_name'].unique():
        old_plan = old_data[old_data['plan_name'] == plan_name].iloc[0]
        new_plan = new_data[new_data['plan_name'] == plan_name].iloc[0]
        
        comparison.append({
            'plan_name': plan_name,
            'walls_match': old_plan['walls_count'] == new_plan['walls_count'],
            'trusses_match': old_plan['trusses_count'] == new_plan['trusses_count'],
            'weight_diff_pct': abs(old_plan['weight'] - new_plan['weight']) / old_plan['weight'] * 100,
            'pass': True  # Calculate based on thresholds
        })
    
    df = pd.DataFrame(comparison)
    
    print(f"\nComparison Results:")
    print(f"Plans compared: {len(df)}")
    print(f"Perfect matches: {df['pass'].sum()}")
    print(f"Discrepancies: {(~df['pass']).sum()}")
    
    df.to_csv("comparison_report.csv", index=False)
    
    return df
```

**Validation Activities:**

1. **Week 4, Day 1-2:** Run both systems in parallel
2. **Week 4, Day 3:** Compare outputs, identify discrepancies
3. **Week 4, Day 4-5:** Fix any data issues, re-validate

---

### **Phase 5: Training & Cutover (Week 5)**

#### **Training Plan**

**Day 1: Scheduler Training**
- System overview
- Creating jobs from plans
- Scheduling workflows
- Crew assignment
- Route optimization

**Day 2: Crew Lead Training**
- Mobile app overview
- Viewing schedules
- Updating job status
- Adding notes/photos

**Day 3: Practice Sessions**
- Real-world scenarios
- Troubleshooting
- Q&A

**Day 4: Go-Live Preparation**
- Final data sync
- Backup procedures
- Support contacts

**Day 5: Go-Live**
- Monitor system closely
- Address issues immediately
- Celebrate success! 🎉

---

#### **Cutover Checklist**
```
PRE-CUTOVER (Day -1)
□ Full system backup
□ Final data validation
□ All users trained
□ Support team on standby
□ Rollback plan ready

CUTOVER DAY
□ 06:00 - Final data sync
□ 07:00 - Freeze legacy system
□ 08:00 - Go-live announcement
□ 09:00 - Monitor first jobs
□ 12:00 - Lunch check-in
□ 17:00 - End-of-day review

POST-CUTOVER (Day +1 to +7)
□ Daily stand-ups
□ Track issues
□ User feedback
□ Performance metrics
□ Celebrate wins

Rollback Plan (If Needed)
python# Script: rollback_import.py

def rollback_import(batch_id, api_url, api_token):
    """Rollback entire import batch"""
    
    response = requests.post(
        f"{api_url}/migration/bat/rollback/{batch_id}",
        headers={'Authorization': f'Bearer {api_token}'}
    )
    
    if response.status_code == 200:
        result = response.json()
        print(f"Rollback successful:")
        print(f"  Plans deleted: {result['plans_deleted']}")
        print(f"  Jobs deleted: {result['jobs_deleted']}")
    else:
        print(f"Rollback failed: {response.text}")

Success Criteria
MetricTargetActualStatusPlans migrated61___⬜Data accuracy100%___⬜Zero downtimeYes___⬜User adoption>90%___⬜Processing time<2 min/job___⬜User satisfaction>8/10___⬜

Post-Migration Support (Week 6+)
Week 6-8: Stabilization

Daily check-ins with users
Monitor system performance
Fix any issues quickly
Gather feedback for improvements

Week 9-12: Optimization

Analyze usage patterns
Optimize slow queries
Add requested features
Train new users

Month 4+: Expansion

Migrate Holt Homes
Onboard Sekisui House
Add new features
Scale infrastructure


Conclusion
You now have a complete blueprint for:

✅ Scheduling module specification with architecture, features, and workflows
✅ Database schemas for all core entities (jobs, schedules, crews, routes, etc.)
✅ API endpoints covering all scheduling operations
✅ Richmond American migration plan with detailed scripts and timelines

Next Steps:

Review this specification with your team
Set up development environment
Begin Phase 1 of Richmond American migration
Build MVP of scheduling module alongside migration

Need help with:

Setting up the database?
Implementing specific API endpoints?
Writing the Python scripts?
Training materials?
Something else?